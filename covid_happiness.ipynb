{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COVID-19 Infections and Happiness\n",
        "This is the notebook for the Python for Economics Project at the London  School of Economics analysing the effect of COVID-19 infections on happiness.\n"
      ],
      "metadata": {
        "id": "M_2dLRCIIqv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "As policy-making during an epidemic is all about making economic tradeoffs, one would like to quantify the gains and losses in the factors a government is is trading off between. The trade-offs to monetary factors and other classical economic factors are well documented, of course. However, the social costs of viral cases less so (among other forms of social costs involved in a pandemic). One may make an attempt to quantify the social costs of the number of cases of such a virus in your country by looking at the causal effect of COVID-19 cases on the average sentiment of how people express themselves online."
      ],
      "metadata": {
        "id": "c3t9AWywlLa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview Project\n",
        "In this project the main goal is to run a regression of the number of COVID-19 infections on the average sentiment of how people express themselves online. You will start by carrying out this analysis for the UK. A clear confounder here are government restrictions to curb the spread of the virus. You will control for this confounder in the regression alongside time-fixed effects that deal with the biases caused by new ways of measuring cases, changes in testing accuracy and availability, among other possible biases.\n",
        "</br></br>\n",
        "To be able to run this final regression, though, you will need to collect the data. This notebook will walk you through the steps associated with this and the final step of running the regression.\n"
      ],
      "metadata": {
        "id": "hpTdOHFalo5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "loLc9eEEVSsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[COVID-19 Infections and Happiness](#scrollTo=M_2dLRCIIqv9)\n",
        "\n",
        ">>[Introduction](#scrollTo=c3t9AWywlLa_)\n",
        "\n",
        ">>[Overview Project](#scrollTo=hpTdOHFalo5C)\n",
        "\n",
        ">>[Table of Contents](#scrollTo=loLc9eEEVSsP)\n",
        "\n",
        ">>[Preparation](#scrollTo=5kqfEAq9S8KC)\n",
        "\n",
        ">>[Data Collection](#scrollTo=Xviu1_5NnsrF)\n",
        "\n",
        ">>>[Loading Datasets](#scrollTo=3gOgjpQpKGoe)\n",
        "\n",
        ">>>[Cleaning Datasets](#scrollTo=Xzmo0WIbKtrk)\n",
        "\n",
        ">>>>[Preparation](#scrollTo=Xzmo0WIbKtrk)\n",
        "\n",
        ">>>[Stringency](#scrollTo=KB_HFVFsdT5H)\n",
        "\n",
        ">>>>[Cases](#scrollTo=6zlemmyxk2JT)\n",
        "\n",
        ">>>[Merging Dataframes](#scrollTo=8UXSCwxHg1Gi)\n",
        "\n",
        ">>>[Average Sentiment](#scrollTo=C_Wv1iKpmdo6)\n",
        "\n",
        ">>>>[Scraping Tweets](#scrollTo=UVZi_0ONKq5p)\n",
        "\n",
        ">>>>[Classifying Tweets](#scrollTo=zOiuZ0v8NSKA)\n",
        "\n",
        ">>[Running Regressions](#scrollTo=1wSD_8JrKslV)\n",
        "\n",
        ">>[Further Exercises](#scrollTo=HIx-MyVcN7Vh)\n",
        "\n",
        ">>[References](#scrollTo=GTQjBctvVLWv)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "nWLxb9_5S6Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preparation\n",
        "First, you will need to install a few libraries for this project. To install a library, write ``!pip install`` in a code block followed by ``name-library`` and the optional ``--quiet`` keyword to suppress the logs. For example, installing the package ``pandas`` can be done by running ``!pip install pandas --quiet`` in a code block.\n",
        "\n",
        "(note: between countries the definitions and methods of confirming cases differs. maybe look at percentual change in infections but then not the absolute size of infections. maybe ONS positive rates)."
      ],
      "metadata": {
        "id": "5kqfEAq9S8KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Install the following packages: pandas, datetime."
      ],
      "metadata": {
        "id": "zM3N6_a9gPMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you have to import the packages you installed. Additionally, import the preinstalled package ``numpy`` as ``np``."
      ],
      "metadata": {
        "id": "bJkVLg9KJyNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Import the installed packages.\n",
        "# Two additional libraries necessary for CSV uploads are already given (no need to install these, they are installed by default on Colabs).\n",
        "from google.colab import files\n",
        "import io"
      ],
      "metadata": {
        "id": "LnuPMkjlJ_5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection\n",
        "We can now start collecting our data."
      ],
      "metadata": {
        "id": "Xviu1_5NnsrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Datasets\n",
        "The data we will use for this analysis will come from the John Hopkins University Center for Systems Science and Engineering, Our World in Data and, of course, Twitter. \n",
        "\n",
        "\n",
        "* The dataset on confirmed cases per country (including the UK) can be found and downloaded [here](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series).\n",
        "* The dataset on COVID-19 government restriction stringency can be found and downloaded [here](https://ourworldindata.org/covid-stringency-index).\n",
        "* We will get into the Tweets later.\n",
        "\n",
        "Once you have downloaded the datasets, you can upload one of them to to Colabs by running the comands below which will store a dataset as a Pandas dataframe (sort of like a spreadsheet). It is a good coding practice to wrap commands like this in a function. Do this and make the function output both datasets in a list. Then, call the function and assign the result to a variable ``dataframes``, storing the two dataframes in a list."
      ],
      "metadata": {
        "id": "3gOgjpQpKGoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This command will prompt you with an upload screen and store the uploaded files in a dictionary.\n",
        "# You can upload multiple files at once.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# This command stores the filenames in a list.\n",
        "filenames = uploaded.keys()\n",
        "\n",
        "# This command selects the filename of the first file in the files you uploaded.\n",
        "filename = filenames[0]\n",
        "\n",
        "# This command stores a dataset in a variable as a Pandas dataframe.\n",
        "dataset = pd.DataFrame(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "# TODO - Create and call the function."
      ],
      "metadata": {
        "id": "OaBrHMLZbnVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Datasets\n",
        "#### Preparation\n",
        "First, it would be nice to have each of the datasets stored in a variable with a corresponding name. Below I show a trick to assign two variables at once. Use this trick to assign your datasets to the variables ``df_cases`` and ``df_stringency``."
      ],
      "metadata": {
        "id": "Xzmo0WIbKtrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lse, ucl = [\"awesome\", \"mwa\"]\n",
        "\n",
        "# TODO - Replicate the trick with the variable names given."
      ],
      "metadata": {
        "id": "PzYAFEKgd8aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Stringency\n",
        "Let's start with the easiest dataset first. Inspect the structure of the dataset by printing the dataframe."
      ],
      "metadata": {
        "id": "KB_HFVFsdT5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - print the dataframe and inspect the structure."
      ],
      "metadata": {
        "id": "ZR-VoJcBKv6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, there are lots of variables and countries of which we do not need the data. Therefore, we would like to drop the redundant entries. Do this by selecting only the date and stringency index values for just the United Kingdom. Overwrite ``df_stringency`` with this transformed dataframe. As a final nit-picky step, reset the index of the dataframe."
      ],
      "metadata": {
        "id": "cUWOFexDgayH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Overwrite the dataframe with the filtered version."
      ],
      "metadata": {
        "id": "xLjnB4JNhwBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to have our data of suitable data types, so it is easiest to work with down the line. For example, we would like the values in our ``date`` column to be of the ``datetime`` data type. Also, we would like the values in our ``stringency_index`` column to be of the ``float`` data type. Check if this is the case and if not, convert the column values to the desired data type."
      ],
      "metadata": {
        "id": "qRnW1do6ioC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Check if the column values data types are correct and convert them if not."
      ],
      "metadata": {
        "id": "0KEX5HEsjbRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cases\n",
        "Now on to the harder dataset. Inspect the structure of the dataset by printing the dataframe."
      ],
      "metadata": {
        "id": "6zlemmyxk2JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - print the dataframe and inspect the structure."
      ],
      "metadata": {
        "id": "0Yvhn1snloQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, there are a lot of countries we do not need the data of. Filter the dataframe to only contain records of the UK (be precise here) and overwrite the original dataframe with the filtered one."
      ],
      "metadata": {
        "id": "D9BK4l8Wl6-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Filter and overwrite the dataframe of cases."
      ],
      "metadata": {
        "id": "pOa0oe0-oDd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some might think we are done now with this dataset, but this dataset has a nasty characteristic. Namely, it is [*wide*](https://en.wikipedia.org/wiki/Wide_and_narrow_data), and quite *wide*, to say the least. Libraries written for Python and other programming languages hardly support this kind of data shape. Therefore, we want to change the shape of the data to the *narrow* format.\n",
        "</br></br>\n",
        "In essence, we would like one column for the date and one column for the confirmed cases. Thus, we need to put the column names in a new variable name called ``date`` and link the corresponding case numbers to the right row.\n",
        "</br></br>\n",
        "Convert the dataframe to a *narrow* format. After understanding the concepts by reading the Wikipedia page linked before, use Pandas' [``melt``](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) implementation to achieve this."
      ],
      "metadata": {
        "id": "oSMiKE20n8Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the dataframe from wide to narrow format."
      ],
      "metadata": {
        "id": "4289VUhkrPdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to convert the date column of data type ``string`` to the data type ``datetime``, because we want to link the time series datasets that we now have parsed to each other and make one big, complete dataset. This is not as easy as it was for the previous dataset, and you will probably find out why."
      ],
      "metadata": {
        "id": "spfwZAvLcQSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the values of the date column to the datetime data type."
      ],
      "metadata": {
        "id": "fPc3C3lvdJTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging Dataframes\n",
        "Now, we would like to merge the dataframes of the COVID-19 cases and COVID-19 policy stringency with eachother, so that for each date that is present in both dataframes we have one observation for the stringency and the number of cases. We will use [Pandas' implementation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) of a merge function."
      ],
      "metadata": {
        "id": "8UXSCwxHg1Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Merge \"df_cases\" with \"df_stringency\" and save the result in a variable called \"df_cases_stringency\"."
      ],
      "metadata": {
        "id": "kgyVsh8zhYkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon inspecting the data, we can see that there are some missing observations for the stringency index, probably because the stringency data does not go as far in time as the cases dataset. To clean this up, we would like to drop these missing values."
      ],
      "metadata": {
        "id": "2v8hP7FCiN-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Drop the missing values in the dataset."
      ],
      "metadata": {
        "id": "OzT7B2VQi597"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having edited data with code that takes a bit to run, you usually want to save your progress by downloading the dataset. (In more advanced projects, you would maybe use a database when using computationally expensive operations). Thus, download your dataset. You can use the previously installed ``files`` Colabs library for this. Download the dataset as ``cases_stringency.csv``. Make sure to exclude the index in the dataframe to CSV conversion step."
      ],
      "metadata": {
        "id": "u6QfU6-3i9kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the dataframe to a CSV file and download it."
      ],
      "metadata": {
        "id": "XdaUbltCldrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Sentiment\n",
        "Now, in the data collection part of this project we only have left the task of collecting data on the average sentiment of how people express themselves online."
      ],
      "metadata": {
        "id": "C_Wv1iKpmdo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scraping Tweets\n",
        "In this section, we will start scraping tweets from the UK in the same time period as variables ``stringency_index`` and ``cases`` are recorded in. In an academic setting, you might prefer to use an official Twitter API, but this can take a while to be admitted to. Additionally, few compromises are made by using an unofficial Twitter scraper.\n",
        "</br></br>\n",
        "If you have left off since everything before this code chunk and your Google Colabs runtime has restarted, you can optionally load the dataset you created in the previous parts with the code below."
      ],
      "metadata": {
        "id": "UVZi_0ONKq5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases_stringency = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "VBqdIVq2fM0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we install a library that allows us to easily scrape tweets from Twitter."
      ],
      "metadata": {
        "id": "IvQYiYAGfLum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape --quiet"
      ],
      "metadata": {
        "id": "PgF_HqaSKsTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the scraping library."
      ],
      "metadata": {
        "id": "qEL1YIa3Vzx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter"
      ],
      "metadata": {
        "id": "LjaDhGWXV8xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will have to define the date range we want to scrape data from before we start scraping tweets. A useful function for this is Pandas' [``date_range``](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html). Define a date range that starts from the earliest date all the way to the last date in your dataframe ``df_cases_stringency``. Store this range of dates in a variable called ``date_range``."
      ],
      "metadata": {
        "id": "FBNc2EKonkt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Define the date range."
      ],
      "metadata": {
        "id": "oqIFugCXoSu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a list to store the tweets in."
      ],
      "metadata": {
        "id": "X7H18ZOUo5ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []"
      ],
      "metadata": {
        "id": "fGRwhtEho7ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the number of tweets to be scraped per day. You can change this number to your liking. I would recommend to try running the code with this number first and possibly increasing it later when sure the code works so wasting computation time can be prevented."
      ],
      "metadata": {
        "id": "7k2LoAa6ovnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_per_day = 10"
      ],
      "metadata": {
        "id": "bUNAgrzTo-tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we want to scrape tweets published from the UK, we need to tell this to our scraper. As it so happens, Twitter uses geographic tags users can choose to attach to their tweets. (There are some problems of representativeness with this approach discussed [here](https://developer.twitter.com/en/docs/tutorials/advanced-filtering-for-geo-data) if you are interested.) The UK tag is ``6416b8512febefc9``. If needed while exploring the **optional** further exercises, you can find tags of other countries via the following Twitter API: ``f\"https://api.twitter.com/1.1/geo/reverse_geocode.json?lat={latitude}&lon={longitude}&granularity=country\"``. You would format the string based on your latitude and longitude variables before plugging the link in your browser or Python API module of choice. Documentation for this API can be found [here](https://developer.twitter.com/en/docs/twitter-api/v1/geo/places-near-location/api-reference/get-geo-reverse_geocode)."
      ],
      "metadata": {
        "id": "5MG-Qse2pfPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can start scraping. To get you started with the functionality of the ``snscrape`` module, I have written a simple piece of code that you can run to understand how this module can be used."
      ],
      "metadata": {
        "id": "pdPZ4bs7pDtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating the working of the \"enumerate\" function.\n",
        "text_list = [\"This\", \"is\", \"how\", \"enumerate\", \"works.\"]\n",
        "for i, text in enumerate(text_list):\n",
        "  print(i, text)\n",
        "\n",
        "# Storing the place ID for the UK.\n",
        "place_id = \"6416b8512febefc9\"\n",
        "\n",
        "# Defining the search query for our Twitter scraper.\n",
        "# The keyword \"lang:en\" will filter for English tweets only.\n",
        "# The keywords \"since:date\" and \"until:date\" define the time range the tweet has to be from.\n",
        "# \"until\" is exclusive, meaning no tweets are scraped from \"2020-05-20\". \"since\" is inclusive.\n",
        "scraped_tweets = sntwitter.TwitterSearchScraper(f\"lang:en place:{place_id} since:2020-05-19 until:2020-05-20\").get_items()\n",
        "\n",
        "# This piece of code will print 5 tweets.\n",
        "# For each iteration in the loop, the scraper will scroll to the next tweet in the feed returned by Twitter.\n",
        "for i, tweet in enumerate(scraped_tweets):\n",
        "  print(tweet.rawContent)\n",
        "  print(tweet.date)\n",
        "  # We will only need the rawContent and date properties of the tweet.\n",
        "  # tweet.rawContent gives the text of the tweet (string)\n",
        "  # tweet.date gives the date and time of the tweet (datetime)\n",
        "  # For more properties, see line 60 and onwards of https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py.\n",
        "\n",
        "  # Stopping the loop.\n",
        "  if i == 4:\n",
        "    break"
      ],
      "metadata": {
        "id": "1YAfwwvwnt3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you hopefully understand how this module works, I want you to write a function called ``scrape_time_range``.  This function will have to return a list of scraped tweets, containing the raw content and date for each tweet in the list.\n",
        "</br></br>\n",
        "This function should take four arguments:\n",
        "1. A list to append the scraped tweets to.\n",
        "2. The place ID.\n",
        "3. The date range.\n",
        "4. The number of tweets to be scraped per day.\n",
        "\n",
        "You want this function to iterate over the dates in the date range first, before defining the search query for that day and scraping the desired number of tweets. Notice that the dates stored in the previously created ``date_range`` are of the data type ``datetime``. They can be converted to strings by using the function [``strftime``](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.strftime.html). You can format the desired output strings with the following keywords:\n",
        "* ``%Y`` which corresponds to YYYY.\n",
        "* ``%m`` which corresponds to mm.\n",
        "* ``%d`` which corresponds to dd.\n",
        "\n",
        "Make sure to take care of the hypens in these dates, too, when converting the date range, as your Twitter search query will be invalid without them. The same applies to the order of the year, month and date in the string.\n",
        "</br></br>\n",
        "**Hint:** wrap the output of ``date_range.strftime()`` in ``list()`` to convert the Numpy object to a Python list, which is more convenient in this instance."
      ],
      "metadata": {
        "id": "gaRxDrlytsTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# 1. Convert the date range to a list of date strings.\n",
        "# 2. Write the scraping function."
      ],
      "metadata": {
        "id": "KjWbM3VUvEod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call your scraping function.\n",
        "**Warning:** with 10 tweets a day this takes about 40 minutes to run and at a later stage the tweet classification task with the best model would take around 6 hours (but you can do this in batches of course)."
      ],
      "metadata": {
        "id": "Z9WZV9Rs2GDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Call it."
      ],
      "metadata": {
        "id": "K5y7RyOP4Tlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to convert the list of tweets to a dataframe and a CSV file to save our progress. Call the dataframe ``df_tweets`` and the CSV file ``tweets.csv``."
      ],
      "metadata": {
        "id": "VTgGgrTe2AIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the list of tweets to a dataframe and a CSV file."
      ],
      "metadata": {
        "id": "8sNpOp3V4Q1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifying Tweets\n",
        "If you have left off before this chunk and your Colabs runtime has refreshed in the meantime, load the dataset below."
      ],
      "metadata": {
        "id": "zOiuZ0v8NSKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the first index of the list of uploaded datasets, as you only upload one.\n",
        "df_tweets = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "Ter9usq7NUal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we need to define a function that cleans the tweets. Namely, users and tweets mentioned in tweets might confuse the classification model that we will use at a later stage. This is possible if usernames and links have words in them that would refer to a certain sentiment but are not used for that purpose in natural text. Thus, we need to neutralise these words in the tweets. Create a function that converts all users (in the form of ``@username``) to \"``@user``\" and all links (in the form of ``https://`` to \"``https``\". Call it ``neutralise_mentions_links`` and make it so that it takes one argument called ``text``.\n",
        "</br></br>\n",
        "Use the ``.split()`` function of strings in Python. Hastags start with \"#\", mentions with \"@\", links with \"https://\". "
      ],
      "metadata": {
        "id": "xtfCW3fQ5Tne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Write a function that removes hashtags and links from a piece of text."
      ],
      "metadata": {
        "id": "MO6uaFVH5lqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the function to all the tweets in the dataframe."
      ],
      "metadata": {
        "id": "K0QIey716y7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Apply the function to all tweets in the dataframe."
      ],
      "metadata": {
        "id": "6EmXTEYM8D_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to classify the sentiment of the tweets in our dataframe. We task an external library with this exercise. The library we will use is ``happytransformer``. First, we install the library."
      ],
      "metadata": {
        "id": "o26msxfu9cX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happytransformer --quiet"
      ],
      "metadata": {
        "id": "8eacejSm90vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we import the text classification functionality from the library we installed."
      ],
      "metadata": {
        "id": "WkSvc5mO97xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import HappyTextClassification"
      ],
      "metadata": {
        "id": "vQ_7Pgwb9_0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, we load the AI model that has been trained on a large dataset of tweets with sentiment labels. We will use this for the analysis. This type of model is called a transformer model which you can read more on [here](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)."
      ],
      "metadata": {
        "id": "RXZMZTg8bJod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happy_tc = HappyTextClassification(model_type=\"BERT\",  model_name=\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=3)"
      ],
      "metadata": {
        "id": "hKzPNn7XbTqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a demonstration of how the model can be used. Now write a function called ``classify_sentiment`` that takes in one argument of ``text`` and outputs the label in numeric form. \n",
        "</br></br>\n",
        "It is important for you to know that the label that the NLP model outputs is one of:\n",
        "* ``LABEL_0``, which corresponds to negative or the numeric form of -1.\n",
        "* ``LABEL_1``, which corresponds to neutral or the numeric form of 0.\n",
        "* ``LABEL_2``, which corresponds to positive or the numeric form of 1.\n",
        "\n",
        "The model outputs one score for each label and returns the label and score corresponding to the label with the highest score."
      ],
      "metadata": {
        "id": "LA8yizwwcU3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = happy_tc.classify_text(\"I think the Python for Economics week is a great initiative.\")\n",
        "print(result.label, result.score)\n",
        "\n",
        "# TODO - Write a function that outputs the label in numeric form."
      ],
      "metadata": {
        "id": "k8pjKAeAdJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the ``sentiment_classifier`` function to the tweets and store the returned labels in a new column called ``sentiment``. **Warning:** doing this can be time intensive. This notebook was tested with 10 tweets per day and it took 6 hours to classify all the tweets scraped over the time range. Try doing this in chunks and downloading the results if you can't run the notebook for 6 hours straight."
      ],
      "metadata": {
        "id": "jHf31Zn_-A6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Apply the sentiment classifier function to the tweets."
      ],
      "metadata": {
        "id": "IKYTWYFH-UP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to calculate the average sentiment for each day. We can drop the column of tweets before we transform the dataframe. Store this new dataframe in a variable called ``df_sentiment``."
      ],
      "metadata": {
        "id": "AS7tT4sv-gjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Drop the column of tweets and transform the dataframe."
      ],
      "metadata": {
        "id": "R2A3TByi-wwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now successfully generated all of our data necessary for the analysis. One last thing to do is to merge the previously merged datasets with our final dataset of average sentiment scores to create the dataframe ``df_covid_happiness``. Download the dataset of the previously merged datasets with the code below if necessary."
      ],
      "metadata": {
        "id": "XGIUa7Jv-zfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases_stringency = upload_datasets()[0]\n",
        "\n",
        "# TODO - Merge the stringency and cases dataset with the sentiment dataset."
      ],
      "metadata": {
        "id": "uxtkaQbI_Q2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we save the generated dataset."
      ],
      "metadata": {
        "id": "zJkAZLOs_wX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"covid_happiness.csv\"\n",
        "df_covid_happiness.to_csv(filename, index=False)\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "z03uKSyn_yWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Regressions\n",
        "In this section you will have to run the following regression and report the results:\n",
        "$average\\_sentiment_t = \\beta positive\\_cases_t + \\gamma stringency_t + \\eta_t + \\varepsilon_t$\n",
        "</br></br>\n",
        "Before running this regression, think of the interpretation of the coefficient $\\beta$ if you run this regression. Would you want to rescale the corresponding variable $positive\\_cases$ with some proportion to improve the interpretability of this regression?\n",
        "</br></br>\n",
        "When interpreting the regression results you should make sure you understand the definitions of the variables used in the regression. For example, the number of confirmed cases for our purposes is actually the 7-day rolling average.\n",
        "</br></br>\n",
        "First we load our dataset if not loaded yet."
      ],
      "metadata": {
        "id": "1wSD_8JrKslV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_covid_happiness = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "2qOTTUTGK3Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight the number of cases by some constant."
      ],
      "metadata": {
        "id": "3XtPB1Z1vGXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Weight the variable to improve the interpretability of the coefficient."
      ],
      "metadata": {
        "id": "nmWsxA_3vIp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now install the required packages for running regressions and generating the corresponding regression tables."
      ],
      "metadata": {
        "id": "W4dlwNVFAUP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install linearmodels --quiet"
      ],
      "metadata": {
        "id": "28Sx08QQAbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then import the installed libraries."
      ],
      "metadata": {
        "id": "WO4OvB2DAer0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from linearmodels.panel import PanelOLS"
      ],
      "metadata": {
        "id": "rRA_fkvfAi30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to use month fixed effects in our regression. We will need to create a variable of month first in order to take this up in our final regression. Create a column that takes a different index for each month-year pair and wrap this in the function ``pd.Categorical()``."
      ],
      "metadata": {
        "id": "RARD-xurvTH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Create a column that takes a different index for each month-year pair."
      ],
      "metadata": {
        "id": "dwdYsRynzjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, save and download the time series dataframe as ``covid_happiness_timeseries.csv``."
      ],
      "metadata": {
        "id": "bcAssm96mVSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Save and download the dataframe."
      ],
      "metadata": {
        "id": "LbrFE92Zz0cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the model. This is not an exercise because I am of the strong opinion that one should not do their econometrics in Python and the time spent searching the code to do this can be seen as suboptimally spent. Namely, documentation on econometric methods in Stata are arguably better documented and is more intuitive to use for people with a background in economics."
      ],
      "metadata": {
        "id": "GDO8ivlwxRLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the date to the index as is required by the package of use.\n",
        "# Also, placing the index of dates in the first column.\n",
        "df_covid_happiness = df_covid_happiness.set_index(\"date\", append=True)\n",
        "df_covid_happiness.index = df_covid_happiness.index.swaplevel(0, 1)\n",
        "\n",
        "# Specifying the model.\n",
        "regression_model = PanelOLS(dependent=df_covid_happiness['sentiment'],\n",
        "                            exog=df_covid_happiness[[\"cases\", \"stringency_index\"]],\n",
        "                            entity_effects=False,\n",
        "                            time_effects=False,\n",
        "                            other_effects=df_covid_happiness['month'])"
      ],
      "metadata": {
        "id": "1I21FhoZxSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the regression."
      ],
      "metadata": {
        "id": "nQuBPKQ2SUf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_results_summary = regression_model.fit(cov_type='clustered', cluster_entity=True).summary"
      ],
      "metadata": {
        "id": "_O1EGvq8SVz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a regression table with the results."
      ],
      "metadata": {
        "id": "5Y3O-SHaSWKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.latex.repr = True\n",
        "print(regression_results_summary)\n",
        "print(regression_results_summary.as_latex())"
      ],
      "metadata": {
        "id": "xKiQ_DjfSZE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing and downloading the regression table in ``LaTeX`` format."
      ],
      "metadata": {
        "id": "eZxVWkwjSalu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_table = open(\"regression_table.tex\", \"w\")\n",
        "regression_table = print(regression_results_summary.as_latex(), file=regression_table)\n",
        "files.download(\"regression_table.tex\")"
      ],
      "metadata": {
        "id": "ZwkHoFHxxk5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Exercises\n",
        "1. One option is to expand this analysis to different countries. Here, it is important to realise that comparing the coefficients of different countries is not justified. Namely, different countries may confirm cases in different ways. Looking at proportional increases in the number of cases will remove this problem, but will disregard the base level of new cases in the country which of course influences the magnitude of the effect on the average sentiment for a given proportional increase in the number of cases.\n",
        "2. Data visualisation: plot the comovement of the variables of interest over time or something else you are interested in seeing that can give a new insight into the problem.\n",
        "3. There may be other confounders present in the regression that I can't think of right now. If you can think of any, download the data for these, clean that data and create a new variable to run the regression with again.\n",
        "4. Scrape tweets from random time intervals to reduce bias induced by Twitter's feed selection methods. Documentation available for some of the keywords necessary in the Twitter search query to do this can be found [here](https://github.com/igorbrigadir/twitter-advanced-search).\n",
        "5. Filtering out spam tweets. You can approach this Natural Language Processing (NLP) problem in various ways, from as advanced as using AI classification methods as looking for duplicated tweets in your list of scraped tweets. You can always combine methods like these, of course.\n",
        "6. Improving the tweet cleaning function.\n",
        "7. Running the regression with different model specifications of how the confounder affects the outcome variable and the dependent variable. Namely, it may be the case that the start of heavy restrictions is not so bad yet, but that people get tired of it the longer these heavy restrictions are in place. You would need to transform the restriction variable to carry out the regression with this different definition of the control variable."
      ],
      "metadata": {
        "id": "HIx-MyVcN7Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## References\n",
        "* Edouard Mathieu, Hannah Ritchie, Lucas Rod√©s-Guirao, Cameron Appel, Charlie Giattino, Joe Hasell, Bobbie Macdonald, Saloni Dattani, Diana Beltekian, Esteban Ortiz-Ospina and Max Roser (2020) - \"Coronavirus Pandemic (COVID-19)\". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/coronavirus' [Online Resource].\n",
        "* Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi.org/10.1016/S1473-3099(20)30120-1. (https://www.sciencedirect.com/science/article/pii/S1473309920301201).\n",
        "* JustAnotherArchivist, snscrape, (2023), GitHub repository, https://github.com/JustAnotherArchivist/snscrape.\n",
        "* igorbrigadir, Twitter Advanced Search, (2023), GitHub repository, https://github.com/igorbrigadir/twitter-advanced-search.\n",
        "* Wide and Narrow Data, Wikipedia, (12 Feb 2023), https://en.wikipedia.org/wiki/Wide_and_narrow_data.\n",
        "* Advanced Filtering for Geo Data, (2023), https://developer.twitter.com/en/docs/tutorials/advanced-filtering-for-geo-data.\n",
        "* Get Places Near a Location, (2023), https://developer.twitter.com/en/docs/twitter-api/v1/geo/places-near-location/api-reference/get-geo-reverse_geocode.\n",
        "\n"
      ],
      "metadata": {
        "id": "GTQjBctvVLWv"
      }
    }
  ]
}